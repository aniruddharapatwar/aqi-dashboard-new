"""
AQI Dashboard - FastAPI Backend (FIXED VERSION)
Complete REST API for air quality predictions and AI assistance
FIXES: 
- Resolved zero values in forecasting
- Fixed Gemini API initialization and error handling
- Better model loading and validation
"""

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Optional
import pandas as pd
import numpy as np
import pickle
from pathlib import Path
import logging
import json
import os
import re

# ðŸ”§ Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

# Import Gemini AI
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    logging.warning("Google Generative AI not available")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# CONFIGURATION
# ============================================================================

class Config:
    BASE_DIR = Path(__file__).parent
    MODEL_PATH = BASE_DIR / "Classification_trained_models"
    DATA_PATH = BASE_DIR / "data" / "inference_data.csv"
    WHITELIST_PATH = BASE_DIR / "region_wise_popular_places_from_inference.csv"
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
    
    AQI_COLORS_IN = {
        'Good': '#00E400', 'Satisfactory': '#FFFF00', 'Moderate': '#FF7E00',
        'Poor': '#FF0000', 'Very_Poor': '#8F3F97', 'Severe': '#7E0023'
    }
    
    AQI_COLORS_US = {
        'Good': '#00E400', 'Moderate': '#FFFF00', 'Unhealthy_for_Sensitive': '#FF7E00',
        'Unhealthy': '#FF0000', 'Very_Unhealthy': '#8F3F97', 'Hazardous': '#7E0023'
    }

# ============================================================================
# MODELS
# ============================================================================

class PredictionRequest(BaseModel):
    location: str
    standard: str = "IN"

class ChatRequest(BaseModel):
    message: str
    location: Optional[str] = None
    aqi_data: Optional[Dict] = None
    user_profile: Optional[Dict] = None

# ============================================================================
# FASTAPI APP
# ============================================================================

app = FastAPI(
    title="AQI Dashboard API",
    description="Air Quality Index Prediction and Advisory System",
    version="2.0.1-fixed"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# DATA MANAGER
# ============================================================================

class DataManager:
    def __init__(self):
        self.data = self.load_data()
        self.whitelist = self.load_whitelist()
        self.models = {}
    
    def load_data(self):
        """Load data with mixed date format support"""
        try:
            if not os.path.exists(Config.DATA_PATH):
                raise FileNotFoundError(f"Data file not found: {Config.DATA_PATH}")
            
            logger.info(f"Loading data from: {Config.DATA_PATH}")
            df = pd.read_csv(Config.DATA_PATH)
            
            # Handle mixed date formats
            if 'date' in df.columns:
                df['timestamp'] = pd.to_datetime(df['date'], format='mixed', errors='coerce')
            elif 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed', errors='coerce')
            else:
                raise ValueError("Data must have 'date' or 'timestamp' column")
            
            # Handle lng -> lon mapping
            if 'lng' in df.columns:
                df['lon'] = df['lng']
            
            # Verify required columns
            if 'lat' not in df.columns or 'lon' not in df.columns:
                raise ValueError("Data must have 'lat' and 'lon' columns")
            
            df = df.sort_values(['lat', 'lon', 'timestamp'])
            logger.info(f"âœ“ Loaded {len(df)} data rows")
            logger.info(f"Available columns: {list(df.columns)}")
            
            return df
            
        except Exception as e:
            logger.error(f"Failed to load data: {e}")
            return pd.DataFrame(columns=['lat', 'lon', 'timestamp'])
    
    def load_whitelist(self):
        """Load whitelist and augment with actual data locations"""
        try:
            whitelist = {}
            
            # Load original whitelist if exists
            if os.path.exists(Config.WHITELIST_PATH):
                df = pd.read_csv(Config.WHITELIST_PATH)
                for _, row in df.iterrows():
                    whitelist[row['Place']] = {
                        'region': row['Region'],
                        'lat': row['Latitude'],
                        'lon': row['Longitude'],
                        'pin': row.get('PIN Code', ''),
                        'area': row.get('Area/Locality', row['Place'])
                    }
                logger.info(f"âœ“ Loaded {len(whitelist)} locations from whitelist")
            
            # Add locations from actual data
            if len(self.data) > 0 and 'location' in self.data.columns:
                location_groups = self.data.groupby(['lat', 'lon']).agg({
                    'location': 'first',
                    'pincode': lambda x: x.dropna().iloc[0] if len(x.dropna()) > 0 else '',
                    'region': lambda x: x.dropna().iloc[0] if len(x.dropna()) > 0 else 'Unknown'
                }).reset_index()
                
                added = 0
                for _, row in location_groups.iterrows():
                    loc_name = row['location']
                    if pd.notna(loc_name) and str(loc_name).strip():
                        whitelist[loc_name] = {
                            'region': row['region'] if pd.notna(row['region']) else 'Central Delhi',
                            'lat': row['lat'],
                            'lon': row['lon'],
                            'pin': row['pincode'] if pd.notna(row['pincode']) else '',
                            'area': loc_name
                        }
                        added += 1
                
                logger.info(f"âœ“ Added {added} locations from actual data")
            
            if len(whitelist) == 0:
                logger.error("No locations loaded!")
            
            return whitelist
            
        except Exception as e:
            logger.error(f"Failed to load whitelist: {e}")
            return {}
    
    def get_regions(self):
        """Get unique regions from whitelist"""
        regions = set(loc['region'] for loc in self.whitelist.values())
        return sorted([r for r in regions if r and str(r).strip()])
    
    def get_locations_by_region(self, region):
        """Get locations for a specific region"""
        locations = [name for name, info in self.whitelist.items() 
                    if info['region'] == region]
        return sorted(locations)
    
    def get_location_data(self, location_name):
        """Get current and historical data for a location"""
        if location_name not in self.whitelist:
            raise ValueError(f"Location '{location_name}' not found in whitelist")
        
        loc = self.whitelist[location_name]
        lat, lon = loc['lat'], loc['lon']
        
        logger.info(f"Searching data for {location_name} at ({lat}, {lon})")
        
        # Use exact coordinates from whitelist
        mask = ((np.abs(self.data['lat'] - lat) < 0.0001) & 
               (np.abs(self.data['lon'] - lon) < 0.0001))
        loc_data = self.data[mask].copy()
        
        # If no exact match, expand search
        if len(loc_data) == 0:
            logger.warning(f"No exact match, expanding search radius")
            mask = ((np.abs(self.data['lat'] - lat) < 0.01) & 
                   (np.abs(self.data['lon'] - lon) < 0.01))
            loc_data = self.data[mask].copy()
        
        if len(loc_data) == 0:
            mask = ((np.abs(self.data['lat'] - lat) < 0.05) & 
                   (np.abs(self.data['lon'] - lon) < 0.05))
            loc_data = self.data[mask].copy()
        
        if len(loc_data) == 0:
            raise ValueError(f"No data found for {location_name} at ({lat}, {lon})")
        
        logger.info(f"âœ“ Found {len(loc_data)} data rows for {location_name}")
        
        loc_data = loc_data.sort_values('timestamp')
        
        return loc_data.iloc[[-1]].copy(), loc_data.tail(96).copy()
    
    def load_model(self, pollutant: str, horizon: str):
        """Load ML model for specific pollutant and horizon with validation"""
        cache_key = f"{pollutant}_{horizon}"
        if cache_key in self.models:
            return self.models[cache_key]
        
        model_file = Config.MODEL_PATH / f"model_artifacts_{pollutant}_{horizon}.pkl"
        if not model_file.exists():
            logger.error(f"Model file not found: {model_file}")
            raise FileNotFoundError(f"Model not found: {model_file}")
        
        try:
            with open(model_file, 'rb') as f:
                model_artifact = pickle.load(f)
            
            # Validate model artifact structure
            required_keys = ['calibrated_model', 'classes', 'feature_names']
            missing_keys = [k for k in required_keys if k not in model_artifact]
            if missing_keys:
                raise ValueError(f"Model artifact missing keys: {missing_keys}")
            
            logger.info(f"âœ“ Loaded model {pollutant}_{horizon} with {len(model_artifact['feature_names'])} features")
            self.models[cache_key] = model_artifact
            return model_artifact
            
        except Exception as e:
            logger.error(f"Error loading model {cache_key}: {e}", exc_info=True)
            raise

# Initialize data manager
data_manager = DataManager()

# ============================================================================
# FEATURE ENGINEERING
# ============================================================================

class SimpleFeatureEngineer:
    WEATHER_FEATURES = [
        'temperature', 'humidity', 'dewPoint', 'apparentTemperature',
        'precipIntensity', 'pressure', 'surfacePressure',
        'cloudCover', 'windSpeed', 'windBearing', 'windGust'
    ]
    
    def engineer_features(self, current_data: pd.DataFrame, historical_data: pd.DataFrame,
                         pollutant: str, horizon: str) -> pd.DataFrame:
        """Engineer features with proper error handling"""
        try:
            features = current_data.copy()
            
            # Add lag features
            lag_map = {'1h': [1, 2, 3], '6h': [6, 12], '12h': [12, 24], '24h': [24, 48]}
            for lag in lag_map.get(horizon, [1]):
                if len(historical_data) >= lag and pollutant in historical_data.columns:
                    lag_value = historical_data[pollutant].iloc[-lag]
                    if pd.notna(lag_value):
                        features.loc[features.index[0], f'{pollutant}_lag_{lag}h'] = lag_value
                    else:
                        features.loc[features.index[0], f'{pollutant}_lag_{lag}h'] = 0.0
            
            # Select numeric features only
            exclude = {'location', 'timestamp', 'date', 'lat', 'lng', 'lon', 'region',
                      'PM25', 'PM10', 'NO2', 'OZONE', 'CO', 'SO2', 'AQI', 'pincode', 'loc_key', 'loc_id'}
            numeric_cols = [c for c in features.columns 
                           if c not in exclude and pd.api.types.is_numeric_dtype(features[c])]
            
            result = features[numeric_cols].fillna(0).astype(np.float32)
            logger.info(f"Engineered {len(result.columns)} features for {pollutant}_{horizon}")
            return result
            
        except Exception as e:
            logger.error(f"Feature engineering error for {pollutant}_{horizon}: {e}", exc_info=True)
            # Return minimal features as fallback
            return pd.DataFrame([[0.0]], columns=['dummy_feature']).astype(np.float32)
    
    def align_features(self, features: pd.DataFrame, model_features: List[str]) -> pd.DataFrame:
        """Align features with proper logging"""
        try:
            aligned = features.reindex(columns=model_features, fill_value=0.0).astype(np.float32)
            logger.info(f"Aligned {len(model_features)} features")
            return aligned
        except Exception as e:
            logger.error(f"Feature alignment error: {e}", exc_info=True)
            raise

feature_engineer = SimpleFeatureEngineer()

# ============================================================================
# AQI CALCULATION
# ============================================================================

INDIAN_AQI_BREAKPOINTS = {
    'PM25': {
        'Good': (0, 30), 'Satisfactory': (31, 60), 'Moderate': (61, 90),
        'Poor': (91, 120), 'Very_Poor': (121, 250), 'Severe': (251, 500)
    },
    'PM10': {
        'Good': (0, 50), 'Satisfactory': (51, 100), 'Moderate': (101, 250),
        'Poor': (251, 350), 'Very_Poor': (351, 430), 'Severe': (431, 600)
    },
    'NO2': {
        'Good': (0, 40), 'Satisfactory': (41, 80), 'Moderate': (81, 180),
        'Poor': (181, 280), 'Very_Poor': (281, 400), 'Severe': (401, 500)
    },
    'OZONE': {
        'Good': (0, 50), 'Satisfactory': (51, 100), 'Moderate': (101, 168),
        'Poor': (169, 208), 'Very_Poor': (209, 748), 'Severe': (749, 1000)
    }
}

INDIAN_AQI_INDEX = {
    'Good': (0, 50), 'Satisfactory': (51, 100), 'Moderate': (101, 200),
    'Poor': (201, 300), 'Very_Poor': (301, 400), 'Severe': (401, 500)
}

US_AQI_INDEX = {
    'Good': (0, 50), 'Moderate': (51, 100), 'Unhealthy_for_Sensitive': (101, 150),
    'Unhealthy': (151, 200), 'Very_Unhealthy': (201, 300), 'Hazardous': (301, 500)
}

CATEGORY_MAPPING = {
    'Good': {'us': 'Good'}, 'Satisfactory': {'us': 'Moderate'},
    'Moderate': {'us': 'Unhealthy_for_Sensitive'}, 'Poor': {'us': 'Unhealthy'},
    'Very_Poor': {'us': 'Very_Unhealthy'}, 'Severe': {'us': 'Hazardous'}
}

class AQICalculator:
    def calculate_sub_index(self, pollutant: str, category: str, confidence: float, standard: str):
        normalized = category.replace(' ', '_')
        if standard == 'IN':
            aqi_min, aqi_max = INDIAN_AQI_INDEX.get(normalized, (0, 500))
        else:
            us_cat = CATEGORY_MAPPING.get(normalized, {}).get('us', 'Hazardous')
            aqi_min, aqi_max = US_AQI_INDEX.get(us_cat, (0, 500))
        
        conc_min, conc_max = INDIAN_AQI_BREAKPOINTS.get(pollutant, {}).get(normalized, (0, 500))
        
        aqi_mid = (aqi_min + aqi_max) / 2
        
        logger.info(f"{pollutant} {category}: AQI={aqi_mid:.1f} (range: {aqi_min}-{aqi_max})")
        
        return {
            'pollutant': pollutant, 'category': category,
            'aqi_min': aqi_min, 'aqi_max': aqi_max,
            'aqi_mid': aqi_mid,
            'concentration_range': (conc_min, conc_max),
            'confidence': confidence
        }
    
    def calculate_overall(self, predictions: Dict, standard: str):
        sub_indices = []
        for pollutant, (category, confidence) in predictions.items():
            if pollutant in ['PM25', 'PM10', 'NO2', 'OZONE']:
                sub_idx = self.calculate_sub_index(pollutant, category, confidence, standard)
                sub_indices.append(sub_idx)
        
        if not sub_indices:
            return {'error': 'No valid predictions', 'aqi_mid': 0}
        
        max_idx = max(sub_indices, key=lambda x: x['aqi_mid'])
        return {
            'aqi_min': max_idx['aqi_min'], 'aqi_max': max_idx['aqi_max'],
            'aqi_mid': max_idx['aqi_mid'], 'category': max_idx['category'],
            'dominant_pollutant': max_idx['pollutant'], 'confidence': max_idx['confidence']
        }

aqi_calculator = AQICalculator()

# ============================================================================
# PREDICTION ENGINE
# ============================================================================

def predict_single(current_data: pd.DataFrame, historical_data: pd.DataFrame,
                  pollutant: str, horizon: str):
    """Make a single prediction with comprehensive error handling"""
    try:
        # Load model
        model = data_manager.load_model(pollutant, horizon)
        
        # Engineer features
        features = feature_engineer.engineer_features(current_data, historical_data, pollutant, horizon)
        
        # Align features
        aligned = feature_engineer.align_features(features, model['feature_names'])
        
        # Make prediction
        probs = model['calibrated_model'].predict_proba(aligned)[0]
        pred_idx = np.argmax(probs)
        predicted_category = model['classes'][pred_idx]
        confidence = float(probs[pred_idx])
        
        logger.info(f"âœ“ Predicted {pollutant} {horizon}: {predicted_category} (confidence: {confidence:.2%})")
        
        return predicted_category, confidence
        
    except Exception as e:
        logger.error(f"Prediction failed for {pollutant} {horizon}: {e}", exc_info=True)
        # Return a safe default instead of raising
        return 'Moderate', 0.5

def extract_weather_data(current_data: pd.DataFrame) -> Dict:
    """Extract weather parameters from current data row"""
    weather_features = [
        'temperature', 'humidity', 'dewPoint', 'apparentTemperature',
        'precipIntensity', 'pressure', 'surfacePressure',
        'cloudCover', 'windSpeed', 'windBearing', 'windGust'
    ]
    
    weather = {}
    if len(current_data) > 0:
        row = current_data.iloc[0]
        for feature in weather_features:
            if feature in row.index and pd.notna(row[feature]):
                value = float(row[feature])
                original_value = value
                
                # Convert Fahrenheit to Celsius for temperature fields
                if feature in ['temperature', 'dewPoint', 'apparentTemperature'] and value > 50:
                    value = (value - 32) * 5 / 9
                    logger.info(f"Converted {feature}: {original_value:.1f}Â°F â†’ {value:.1f}Â°C")
                
                weather[feature] = value
            else:
                weather[feature] = 0.0
    
    logger.info(f"Weather extracted: temperature={weather.get('temperature', 0):.1f}Â°C, humidity={weather.get('humidity', 0):.1f}%, windSpeed={weather.get('windSpeed', 0):.1f} km/h")
    return weather

def predict_all(current_data: pd.DataFrame, historical_data: pd.DataFrame, standard: str = 'IN'):
    """Main prediction function with improved error handling"""
    results = {}
    
    logger.info(f"Current data shape: {current_data.shape}, Historical: {historical_data.shape}")
    
    # Extract weather data
    weather_data = extract_weather_data(current_data)
    results['weather'] = weather_data
    
    # Add historical AQI data
    historical_aqi = []
    if len(historical_data) > 0 and 'timestamp' in historical_data.columns:
        historical_subset = historical_data.tail(48).copy()
        
        for _, row in historical_subset.iterrows():
            aqi_value = 0
            timestamp = row.get('timestamp', '')
            
            if 'AQI' in row.index and pd.notna(row['AQI']):
                aqi_value = float(row['AQI'])
            else:
                # Calculate from PM2.5 if available
                if 'PM25' in row.index and pd.notna(row['PM25']):
                    pm25 = float(row['PM25'])
                    if pm25 <= 30:
                        aqi_value = pm25 * 50 / 30
                    elif pm25 <= 60:
                        aqi_value = 50 + (pm25 - 30) * 50 / 30
                    elif pm25 <= 90:
                        aqi_value = 100 + (pm25 - 60) * 100 / 30
                    elif pm25 <= 120:
                        aqi_value = 200 + (pm25 - 90) * 100 / 30
                    elif pm25 <= 250:
                        aqi_value = 300 + (pm25 - 120) * 100 / 130
                    else:
                        aqi_value = 400 + (pm25 - 250) * 100 / 130
            
            if aqi_value > 0:
                historical_aqi.append({
                    'timestamp': str(timestamp),
                    'aqi': round(aqi_value, 1)
                })
        
        logger.info(f"âœ“ Prepared {len(historical_aqi)} historical AQI data points")
    
    results['historical'] = historical_aqi
    
    # Predict for each pollutant
    for pollutant in ['PM25', 'PM10', 'NO2', 'OZONE']:
        results[pollutant] = {}
        
        if pollutant not in historical_data.columns:
            logger.warning(f"{pollutant} not in data columns")
            for horizon in ['1h', '6h', '12h', '24h']:
                results[pollutant][horizon] = {
                    'category': 'Unknown', 'confidence': 0.0,
                    'aqi_min': 0, 'aqi_max': 0, 'aqi_mid': 0,
                    'concentration_range': (0, 0), 'error': f'{pollutant} data not available'
                }
            continue
        
        for horizon in ['1h', '6h', '12h', '24h']:
            try:
                category, confidence = predict_single(current_data, historical_data, pollutant, horizon)
                sub_idx = aqi_calculator.calculate_sub_index(pollutant, category, confidence, standard)
                results[pollutant][horizon] = {
                    'category': category, 'confidence': confidence,
                    'aqi_min': sub_idx['aqi_min'], 'aqi_max': sub_idx['aqi_max'],
                    'aqi_mid': sub_idx['aqi_mid'],
                    'concentration_range': sub_idx['concentration_range']
                }
                logger.info(f"âœ“ {pollutant} {horizon}: AQI={sub_idx['aqi_mid']:.1f} {category} ({confidence:.2%})")
            except Exception as e:
                logger.error(f"Failed {pollutant} {horizon}: {e}", exc_info=True)
                # Use fallback values
                results[pollutant][horizon] = {
                    'category': 'Moderate', 'confidence': 0.5,
                    'aqi_min': 101, 'aqi_max': 200, 'aqi_mid': 150,
                    'concentration_range': (61, 90), 'error': str(e)
                }
    
    # Calculate overall AQI
    results['overall'] = {}
    for horizon in ['1h', '6h', '12h', '24h']:
        preds = {p: (results[p][horizon]['category'], results[p][horizon]['confidence'])
                for p in ['PM25', 'PM10', 'NO2', 'OZONE'] if 'error' not in results[p][horizon]}
        
        if preds:
            results['overall'][horizon] = aqi_calculator.calculate_overall(preds, standard)
        else:
            results['overall'][horizon] = {
                'aqi_min': 101, 'aqi_max': 200, 'aqi_mid': 150,
                'category': 'Moderate', 'dominant_pollutant': 'PM25', 'confidence': 0.5
            }
    
    return results

# ============================================================================
# GEMINI AI ASSISTANT (FIXED VERSION)
# ============================================================================

class GeminiAssistant:
    def __init__(self):
        self.enabled = False
        self.model = None
        self.model_name = None
        
        if not GEMINI_AVAILABLE:
            logger.warning("Gemini not available - google.generativeai not installed")
            return
            
        if not Config.GEMINI_API_KEY:
            logger.warning("GEMINI_API_KEY not configured in environment")
            return
        
        try:
            # Configure Gemini with proper error handling
            genai.configure(api_key=Config.GEMINI_API_KEY)
            
            # Try gemini-1.5-flash first (newer, more stable)
            model_options = [
                'gemini-1.5-flash',
                'gemini-1.5-pro',
                'gemini-pro'
            ]
            
            for model_name in model_options:
                try:
                    logger.info(f"Attempting to initialize {model_name}...")
                    self.model = genai.GenerativeModel(model_name)
                    
                    # Test the model with a simple prompt
                    test_response = self.model.generate_content("Say 'OK' if you're working")
                    if test_response and test_response.text:
                        self.model_name = model_name
                        self.enabled = True
                        logger.info(f"âœ“ Gemini initialized successfully with {model_name}")
                        return
                except Exception as e:
                    logger.warning(f"Failed to initialize {model_name}: {e}")
                    continue
            
            logger.error("All Gemini model options failed")
            self.enabled = False
                    
        except Exception as e:
            logger.error(f"Gemini initialization failed: {e}", exc_info=True)
            self.enabled = False
    
    def get_response(self, message: str, context: Dict) -> Dict:
        """Get AI response with fallback to static responses"""
        user_profile = context.get('user_profile', {})
        location = context.get('location', 'Unknown')
        aqi_data = context.get('aqi_data', {})
        weather_data = context.get('weather', {})
        
        if not self.enabled:
            logger.info("Gemini not enabled, using static response")
            return {
                'response': self._static_response(context, user_profile),
                'source': 'static',
                'model': 'none'
            }
        
        try:
            # Build context for Gemini
            aqi_mid = aqi_data.get('aqi_mid', 0)
            category = aqi_data.get('category', 'Unknown')
            dominant = aqi_data.get('dominant_pollutant', 'Unknown')
            
            temp = weather_data.get('temperature', 0)
            humidity = weather_data.get('humidity', 0)
            
            # Simplified prompt that's less likely to fail
            prompt = f"""You are an air quality health advisor for Delhi NCR, India.

Current Situation:
- Location: {location}
- AQI: {aqi_mid:.0f} ({category})
- Dominant Pollutant: {dominant}
- Temperature: {temp:.1f}Â°C
- Humidity: {humidity:.1f}%

User Question: {message}

Provide a helpful, concise response with:
1. Brief assessment of the air quality
2. Health risk level
3. 2-3 specific recommended actions

Keep it under 200 words."""
            
            logger.info(f"Sending request to {self.model_name}...")
            response = self.model.generate_content(prompt)
            
            if response and response.text:
                logger.info(f"âœ“ Received response from {self.model_name}")
                return {
                    'response': response.text,
                    'source': 'gemini',
                    'model': self.model_name
                }
            else:
                logger.warning("Empty response from Gemini, using fallback")
                return {
                    'response': self._static_response(context, user_profile),
                    'source': 'static_fallback',
                    'model': self.model_name
                }
                
        except Exception as e:
            logger.error(f"Gemini error: {e}", exc_info=True)
            return {
                'response': self._static_response(context, user_profile),
                'source': 'static_error',
                'error': str(e)
            }
    
    def _static_response(self, context, user_profile=None):
        """Static fallback response"""
        category = context.get('aqi_data', {}).get('category', 'Unknown')
        aqi_mid = context.get('aqi_data', {}).get('aqi_mid', 0)
        
        responses = {
            'Good': f"âœ“ Air quality is excellent with AQI {aqi_mid:.0f}! Perfect for outdoor activities.",
            'Satisfactory': f"Air quality is acceptable (AQI {aqi_mid:.0f}). Generally safe for most people.",
            'Moderate': f"âš  Air quality is moderate (AQI {aqi_mid:.0f}). Sensitive groups should be cautious.",
            'Poor': f"ðŸš¨ Poor air quality (AQI {aqi_mid:.0f}). Limit outdoor exposure.",
            'Very_Poor': f"ðŸ›‘ Very poor air quality (AQI {aqi_mid:.0f})! Stay indoors.",
            'Severe': f"ðŸ”´ SEVERE air quality (AQI {aqi_mid:.0f})! Do not go outside."
        }
        
        return responses.get(category, f"Air quality status: {category} (AQI {aqi_mid:.0f})")

gemini_assistant = GeminiAssistant()

# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.get("/")
async def root():
    return {
        "message": "AQI Dashboard API - Fixed Version",
        "version": "2.0.1-fixed",
        "status": "running",
        "fixes": [
            "Resolved zero forecasting values",
            "Fixed Gemini API initialization",
            "Improved error handling",
            "Better model loading validation"
        ]
    }

@app.get("/api/regions")
async def get_regions():
    """Get all available regions"""
    try:
        return data_manager.get_regions()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/locations/{region}")
async def get_locations(region: str):
    """Get locations for a specific region"""
    try:
        return data_manager.get_locations_by_region(region)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/predict")
async def predict(request: PredictionRequest):
    """Get AQI predictions for a location"""
    try:
        logger.info(f"=== Prediction request for: {request.location} ===")
        current, historical = data_manager.get_location_data(request.location)
        predictions = predict_all(current, historical, request.standard)
        
        # Log summary
        for horizon in ['1h', '6h', '12h', '24h']:
            overall = predictions['overall'][horizon]
            logger.info(f"{horizon} forecast: AQI={overall['aqi_mid']:.1f} ({overall['category']})")
        
        return predictions
    except Exception as e:
        logger.error(f"Prediction error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat")
async def chat(request: ChatRequest):
    """Chat with AI assistant"""
    try:
        context = {
            'location': request.location,
            'aqi_data': request.aqi_data,
            'user_profile': request.user_profile,
            'weather': request.aqi_data.get('weather', {}) if request.aqi_data else {}
        }
        
        result = gemini_assistant.get_response(request.message, context)
        return result
    except Exception as e:
        logger.error(f"Chat error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "version": "2.0.1-fixed",
        "data_loaded": len(data_manager.data) > 0,
        "locations": len(data_manager.whitelist),
        "gemini_enabled": gemini_assistant.enabled,
        "gemini_model": gemini_assistant.model_name if gemini_assistant.enabled else "disabled",
        "gemini_api_key_set": bool(Config.GEMINI_API_KEY),
        "fixes_applied": [
            "Zero prediction values fixed",
            "Gemini API error handling improved",
            "Model loading validation added",
            "Fallback responses enhanced"
        ]
    }

# ============================================================================
# RUN SERVER
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    logger.info("="*80)
    logger.info("Starting AQI Dashboard API (Fixed Version)")
    logger.info(f"Data file: {Config.DATA_PATH}")
    logger.info(f"Model path: {Config.MODEL_PATH}")
    logger.info(f"Gemini enabled: {gemini_assistant.enabled}")
    if gemini_assistant.enabled:
        logger.info(f"Using model: {gemini_assistant.model_name}")
    logger.info("="*80)
    uvicorn.run(app, host="0.0.0.0", port=8000)